{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanscripting Video Files From TikTok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies\n",
    "We need moviepy to convert the videos to mp3\n",
    "We need openai-whisper to tanscript the files to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install moviepy\n",
    "%pip install -U openai-whisper\n",
    "%pip install faster-whisper \n",
    "%pip install pandas \n",
    "%pip install requests\n",
    "%pip install moviepy\n",
    "%pip install -U spacy\n",
    "%pip install langdetect\n",
    "%pip install -U yt-dlp\n",
    "!brew install ffmpeg\n",
    "pip install gradio_client\n",
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your notebook to the right directrory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/noursafadi/Documents/Uni/Parsons-Spring-25/MajorStudio02/Thesis/tiktok-scraper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noursafadi/.pyenv/versions/3.10.12/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd \"/Users/noursafadi/Documents/Uni/Parsons-Spring-25/MajorStudio02/Thesis/tiktok-scraper\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import *\n",
    "import whisper\n",
    "import csv\n",
    "import requests\n",
    "from faster_whisper import WhisperModel\n",
    "import httpx\n",
    "import json\n",
    "from os import listdir, path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a pandas DataFrame\n",
    "JSON_PATH = \"data/dataset_youtube_vids.json\"\n",
    "with open(JSON_PATH, \"r\") as file: \n",
    "    df = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##index each row in the original dataset \n",
    "##index each downloaded audio\n",
    "\n",
    "#Step 01 \n",
    "# TODO: Loop throug the dataset \n",
    "# TODO: add a new column index starting 0 for each row \n",
    "\n",
    "# TODO: Loop through the dataset \n",
    "    # TODO: Get URL for each video\n",
    "    # TODO: Get index \n",
    "    # TODO: name the audio index-video\n",
    "\n",
    "#Step 02\n",
    "# TODO: Whisper transcript Loop throug the audio files \n",
    "# TODO: Extraxt the index name \n",
    "# TODO: Match it with index in the original DF\n",
    "# TODO: Append the transcript to a new column witht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop throug the dataset \n",
    "# TODO: add a new column index starting 0 for each row \n",
    "for  idx, row in enumerate(df):\n",
    "    row[\"index\"] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTube to Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop through the dataset \n",
    "for row in df:\n",
    "    # TODO: Get URL for each video\n",
    "    print(row[\"url\"])\n",
    "    url = row[\"url\"]\n",
    "    # TODO: Get index \n",
    "    print(row[\"index\"])\n",
    "    id = row[\"index\"]\n",
    "    # TODO: name the audio index-video\n",
    "    ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'outtmpl': f'audio_files/{id}-audio.%(ext)s'\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint with 118 transcribed files\n"
     ]
    }
   ],
   "source": [
    "# Load the latest checkpoint\n",
    "try:\n",
    "    with open('transcribed_data.pkl', 'rb') as f:\n",
    "        df_copy = pickle.load(f)\n",
    "    transcribed_count = sum(1 for item in df_copy if 'transcript' in item and item['transcript'])\n",
    "    print(f\"Loaded checkpoint with {transcribed_count} transcribed files\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "    df_copy = df.copy()  # This requires df to be defined first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gradio_client hosted on Kaggle to use GPU to run Whisper AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client\n",
    "from gradio_client.utils import handle_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://c79944888f1ae46d67.gradio.live/ âœ”\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not fetch config for https://c79944888f1ae46d67.gradio.live/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://c79944888f1ae46d67.gradio.live/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m audio_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_files/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m audio_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m listdir(audio_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/gradio_client/client.py:153\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, src, hf_token, max_workers, verbose, auth, httpx_kwargs, headers, download_files, ssl_verify, _skip_components)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_login(auth)\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mws\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse_v2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mws\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m api_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/gradio_client/client.py:908\u001b[0m, in \u001b[0;36mClient._get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m r \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc,\n\u001b[1;32m    902\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttpx_kwargs,\n\u001b[1;32m    906\u001b[0m )\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m r\u001b[38;5;241m.\u001b[39mis_success:\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not fetch config for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# some basic regex to extract the config\u001b[39;00m\n\u001b[1;32m    910\u001b[0m result \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.gradio_config = (.*?);[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*</script>\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not fetch config for https://c79944888f1ae46d67.gradio.live/"
     ]
    }
   ],
   "source": [
    "client = Client(\"https://c79944888f1ae46d67.gradio.live/\")\n",
    "\n",
    "audio_folder = \"audio_files/\"\n",
    "audio_files = [f for f in listdir(audio_folder) if f.endswith(\".mp3\")]\n",
    "\n",
    "# Track progress - now using the full length\n",
    "total_files = len(audio_files)\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "# Loop through ALL audio files (removed the [:100] slice)\n",
    "for fname in audio_files:\n",
    "    file_path = path.join(audio_folder, fname)\n",
    "    number = int(fname.split(\"-\")[0])\n",
    "    \n",
    "    # Find matching item in df_copy\n",
    "    matching_item = next((item for item in df_copy if item[\"index\"] == number), None)\n",
    "    \n",
    "    # Skip if already transcribed\n",
    "    if matching_item and \"transcript\" in matching_item and matching_item[\"transcript\"]:\n",
    "        print(f\"Skipping {fname} - already transcribed\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "        \n",
    "    if matching_item:\n",
    "        print(f\"Processing {fname} ({processed+1}/{total_files})\")\n",
    "        try:\n",
    "            result = client.predict(\n",
    "                audio=handle_file(file_path),\n",
    "                api_name=\"/predict\"\n",
    "            )\n",
    "            matching_item[\"transcript\"] = result\n",
    "            print(f\"Transcribed: {result[:100]}...\")  # Print start of transcript\n",
    "            \n",
    "            # Save checkpoint every 10 files (or adjust as needed for larger datasets)\n",
    "            processed += 1\n",
    "            if processed % 10 == 0:\n",
    "                with open('transcribed_data.pkl', 'wb') as f:\n",
    "                    pickle.dump(df_copy, f)\n",
    "                print(f\"Checkpoint saved ({processed}/{total_files})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")\n",
    "            # Save on error to prevent losing progress\n",
    "            with open('transcribed_data_error.pkl', 'wb') as f:\n",
    "                pickle.dump(df_copy, f)\n",
    "            print(\"Progress saved after error\")\n",
    "    else:\n",
    "        print(f\"No matching index found for {fname}\")\n",
    "\n",
    "print(f\"Finished processing. Transcribed: {processed}, Skipped: {skipped}\")\n",
    "\n",
    "# Save final results\n",
    "with open('transcribed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(df_copy, f)\n",
    "\n",
    "with open('transcribed_data.json', 'w') as f:\n",
    "    json.dump(df_copy, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transcribed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(df_copy, f)\n",
    "\n",
    "with open('transcribed_data.json', 'w') as f:\n",
    "    json.dump(df_copy, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv 3.10)",
   "language": "python",
   "name": "pyenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
